[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "intelligence",
    "section": "",
    "text": "Introduction to Intelligence\n\n\nIntelligence\nWARNING: THIS IS A WORK IN PROGRESS AND MAY NOT BE ENTIRELY SCIENTIFICALLY ACCURATE. PLEASE DO NOT QUOTE ME ON THIS STUFF UNTIL AT LEAST THIS DOCUMENT IS FINISHED\n\n\nWhat is intelligence?\n(Phenotypically useful definition) “Intelligence is a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.”\n(Statistically useful definition) “Intelligence is at the pinnacle of the hierarchical model of cognitive abilities that includes a middle level of group factors, such as the cognitive domains of verbal and spatial abilities and memory, and a third level of specific tests and their associated narrow cognitive skills.”"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alfonso, V. C., D. P. Flanagan, S. Radwan, D. P. Flanagan, and P. L.\nHarrison. 2005. “The Impact of the Cattell-Horn-Carroll Theory on\nTest Development and Interpretation of Cognitive and Academic\nAbilities.” In Contemporary Intellectual Assessment:\nTheories, Tests, and Issues, 185–202.\n\n\nBeal, A Lynne. 2006. “Review of Contemporary Intellectual\nAssessment, Theories, Tests and Issues.”\n\n\nBRESLAU, JOSHUA, SERGIO AGUILAR-GAXIOLA, KENNETH S. KENDLER, MAXWELL SU,\nDAVID WILLIAMS, and RONALD C. KESSLER. 2006. “Specifying\nRace-Ethnic Differences in Risk for Psychiatric Disorder in a USA\nNational Sample.” Psychological Medicine 36 (1): 57–68.\nhttps://doi.org/10.1017/S0033291705006161.\n\n\nCanivez, G. L. 2017. “Test Review of Woodcock-Johnson® IV.”\nIn The Twentieth Mental Measurements Yearbook, edited by J. F.\nCarlson, K. F. Geisinger, and J. L. Jonson.\n\n\nCanivez, Gary, and Marley Watkins. 2016. “Review of the Wechsler\nIntelligence Scale for Children–Fifth Edition: Critique, Commentary, and\nIndependent Analyses.” In, 683–702.\n\n\nCarroll, J. B., C. J. B, and Cambridge University Press. 1993. Human\nCognitive Abilities: A Survey of Factor-Analytic Studies. Human\nCognitive Abilities: A Survey of Factor-Analytic Studies, no. 1.\nCambridge University Press. https://books.google.com/books?id=jp9dt4\\_0\\_cIC.\n\n\nCarroll, John. 2003. “The Higher-Stratum Structure of Cognitive\nAbilities: Current Evidence Supports g and about Ten Broad\nFactors.” In The Scientific Study of General Intelligence:\nTribute to Arthur R. Jensen, 5–21. https://doi.org/10.1016/B978-008043793-4/50036-2.\n\n\nClimie, Emma A., and Kristin Rostad. 2011. “Test Review: Wechsler\nAdult Intelligence Scale.” Journal of Psychoeducational\nAssessment 29 (6): 581–86. https://doi.org/10.1177/0734282911408707.\n\n\nColeman, Karen J., Christine Stewart, Beth E. Waitzfelder, John E.\nZeber, Leo S. Morales, Ameena T. Ahmed, Brian K. Ahmedani, et al. 2016.\n“Racial-Ethnic Differences in Psychiatric Diagnoses and Treatment\nAcross 11 Health Care Systems in the Mental Health Research\nNetwork.” Psychiatric Services 67 (7): 749–57. https://doi.org/10.1176/appi.ps.201500217.\n\n\nCormier, Damien C., Kathleen E. Kennedy, and Alexandra M. Aquilina.\n2016. “Test Review: Wechsler Intelligence Scale for Children,\nFifth Edition: Canadian 322 (WISC-VCDN) by d. Wechsler.”\nCanadian Journal of School Psychology 31 (4): 322–34. https://doi.org/10.1177/0829573516648941.\n\n\nCurtin, Sally C, Kamiah A Brown, and Mariah E Jordan. 2022.\n“Suicide Rates for the Three Leading Methods by Race and Ethnicity\n: United States, 2000–2020.” NCHS Data Brief ; No. 450.\nhttps://dx.doi.org/10.15620/cdc:121798; National Center\nfor Health Statistics.\n\n\nDeary, Ian. 1998. “Differences in Mental Abilities.”\nBMJ (Clinical Research Ed.) 317 (7174): 1701–3. https://doi.org/10.1136/bmj.317.7174.1701.\n\n\n———. 2001. Intelligence: A Very Short Introduction. https://doi.org/10.1093/actrade/9780192893215.001.0001.\n\n\nDetterman, Douglas K. 2014. “You Should Be Teaching\nIntelligence!” Intelligence 42: 148–51.\nhttps://doi.org/https://doi.org/10.1016/j.intell.2013.07.021.\n\n\nFlanagan, Dawn P., and Shauna G. Dixon. 2014. “The\nCattell-Horn-Carroll Theory of Cognitive Abilities.” In\nEncyclopedia of Special Education. John Wiley & Sons, Ltd.\nhttps://doi.org/https://doi.org/10.1002/9781118660584.ese0431.\n\n\nFlore, Paulette C., and Jelte M. Wicherts. 2015. “Does Stereotype\nThreat Influence Performance of Girls in Stereotyped Domains? A\nMeta-Analysis.” Journal of School Psychology 53 (1):\n25–44. https://doi.org/https://doi.org/10.1016/j.jsp.2014.10.002.\n\n\nGoodenough, FL. 1926. “Racial Differences in the Intelligence of\nSchool Children.” Journal of Experimental Psychology 9\n(4): 388–97. https://doi.org/10.1037/h0073325.\n\n\nGottfredson, Linda S. 1997. “Mainstream Science on Intelligence:\nAn Editorial with 52 Signatories, History, and Bibliography.”\nIntelligence 24 (1): 13–23. https://doi.org/https://doi.org/10.1016/S0160-2896(97)90011-8.\n\n\nGustafsson, Jan-Eric. 1984. “A Unifying Model for the Structure of\nIntellectual Abilities.” Intelligence 8 (3): 179–203.\nhttps://doi.org/https://doi.org/10.1016/0160-2896(84)90008-4.\n\n\nHunter, John E, and Frank L Schmidt. 2000. “Racial and Gender Bias\nin Ability and Achievement Tests: Resolving the Apparent\nParadox.” Psychology, Public Policy, and Law 6 (1):\n151–58. https://doi.org/10.1037/1076-8971.6.1.151.\n\n\nJanzen, Henry L., John E. Obrzut, and Christopher W. Marusiak. 2004.\n“Test Review: Roid, g. H. (2003). Stanford-Binet Intelligence\nScales, Fifth Edition (SB:v). Itasca, IL: Riverside Publishing.”\nCanadian Journal of School Psychology 19 (1-2): 235–44. https://doi.org/10.1177/082957350401900113.\n\n\nJensen, A. R. 1980. Bias in Mental Testing. Free Press. https://books.google.com/books?id=wJR9AAAAMAAJ.\n\n\nJensen, Arthur R., and Frank C. J. McGurk. 1987. “Black-White Bias\nin ‘Cultural’ and ‘Noncultural’ Test\nItems.” Personality and Individual Differences 8 (3):\n295–301. https://doi.org/https://doi.org/10.1016/0191-8869(87)90029-8.\n\n\nJewsbury, Paul, Stephen Bowden, and Kevin Duff. 2017. “The\nCattell–Horn–Carroll Model of Cognition for Clinical Assessment.”\nJournal of Psychoeducational Assessment 35 (6): 547–67. https://doi.org/10.1177/0734282916651360.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and\nIrving I Gottesman. 2004. “Just One g: Consistent Results from\nThree Test Batteries.” Intelligence 32 (1): 95–107.\nhttps://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008.\n“Still Just 1 g: Consistent Results from Five Test\nBatteries.” Intelligence 36 (1): 81–95.\nhttps://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nKeith, Timothy, John Kranzler, and Dawn Flanagan. 2001. “What Does\nthe Cognitive Assessment System (CAS) Measure? Joint Confirmatory Factor\nAnalysis of the CAS and the Woodcock-Johnson Tests of Cognitive\nAbility.” School Psychology Review 30 (March): 89–119.\nhttps://doi.org/10.1080/02796015.2001.12086102.\n\n\nKrueger, Patrick M, Jarron M Saint Onge, and Virginia W Chang. 2011.\n“Race/Ethnic Differences in Adult Mortality: The Role of Perceived\nStress and Health Behaviors.” Social Science &\nMedicine 73 (9): 1312–22. https://doi.org/10.1016/j.socscimed.2011.08.007.\n\n\nLubinski, David. 2004. “Introduction to the Special Section on\nCognitive Abilities: 100 Years After Spearman’s (1904) \"’General\nIntelligence,’ Objectively Determined and Measured\".” Journal\nof Personality and Social Psychology 86 (1): 96–111. https://doi.org/10.1037/0022-3514.86.1.96.\n\n\nMackintosh, Nicholas. 2011. IQ and Human Intelligence. OUP\nOxford. https://books.google.com/books?id=BcKcAQAAQBAJ.\n\n\nMadle, R. A. 2017. “[Test Review of Woodcock-Johnson® IV].”\nIn The Twentieth Mental Measurements Yearbook, edited by J. F.\nCarlson, K. F. Geisinger, and J. L. Jonson.\n\n\nMcGrew, Kevin S, DP Flanagan, JL Genshaft, and PL Harrison. 2005.\nContemporary Intellectual Assessment: Theories, Tests, and\nIssues. The Guilford Press New York, NY, USA:\n\n\nNeisser, Ulric, Gwyneth Boodoo, Thomas J. Bouchard Jr., A. Wade Boykin,\nNathan Brody, Stephen J. Ceci, Diane F. Halpern, et al. 1996.\n“Intelligence: Knowns and Unknowns.” American\nPsychologist 51 (2): 77–101. https://doi.org/10.1037/0003-066X.51.2.77.\n\n\nPorteus, SD. 1965. Porteus Maze Test; Fifty Years’ Application.\nPacific Books.\n\n\nReeve, Charlie L., and Jennifer E. Charles. 2008. “Survey of\nOpinions on the Primacy of g and Social Consequences of Ability Testing:\nA Comparison of Expert and Non-Expert Views.”\nIntelligence 36 (6): 681–88. https://doi.org/https://doi.org/10.1016/j.intell.2008.03.007.\n\n\n“Retraction Notice.” 2021. Personality and Social\nPsychology Bulletin 47 (1): 161–61. https://doi.org/10.1177/0146167220973962.\n\n\nReynolds, Cecil R. 2000. “Why Is Psychometric Research on Bias in\nMental Testing so Often Ignored?” Psychology, Public Policy,\nand Law 6 (1): 144–50. https://doi.org/10.1037/1076-8971.6.1.144.\n\n\nReynolds, Cecil R. 1995. “Test Bias and the Assessment of\nIntelligence and Personality.” In International Handbook of\nPersonality and Intelligence, edited by Donald H. Saklofske and\nMoshe Zeidner, 545–73. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4757-5571-8_25.\n\n\n———. 1998. “10.03 - Cultural Bias in Testing of Intelligence and\nPersonality.” In Comprehensive Clinical Psychology,\nedited by Alan S. Bellack and Michel Hersen, 53–92. Oxford: Pergamon.\nhttps://doi.org/https://doi.org/10.1016/B0080-4270(73)00105-X.\n\n\nReynolds, Cecil R., and Robert T. Brown. 1984. “Bias in Mental\nTesting.” In Perspectives on Bias in Mental Testing,\nedited by Cecil R. Reynolds and Robert T. Brown, 1–39. Boston, MA:\nSpringer US. https://doi.org/10.1007/978-1-4684-4658-6_1.\n\n\nReynolds, Cecil R., and Lisa A. Suzuki. 2012. “Bias in\nPsychological Assessment.” In Handbook of Psychology, Second\nEdition. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118133880.hop210004.\n\n\nRiolo, Stephanie A., Tuan Anh Nguyen, John F. Greden, and Cheryl A.\nKing. 2005. “Prevalence of Depression by Race/Ethnicity: Findings\nfrom the National Health and Nutrition Examination Survey III.”\nAmerican Journal of Public Health 95 (6): 998–1000. https://doi.org/10.2105/AJPH.2004.047225.\n\n\nSchimmack, Ulrich. 2017. “Hidden Figures: Replication Failures in\nthe Stereotype Threat Literature.” https://replicationindex.com/2017/04/07/hidden-figures-replication-failures-in-the-stereotype-threat-literature/.\n\n\nSchneider, W Joel, Kevin S McGrew, DP Flanagan, and PL Harrison. 2012.\n“Contemporary Intellectual Assessment: Theories, Tests, and\nIssues.” Institute for Applied Psychometrics (IAP).\n\n\nShewach, Oren R, Paul R Sackett, and Sander Quint. 2019.\n“Stereotype Threat Effects in Settings with Features Likely Versus\nUnlikely in Operational Test Settings: A Meta-Analysis.”\nJournal of Applied Psychology 104 (12): 1514–34. https://doi.org/10.1037/apl0000420.\n\n\nStauffer, Joseph M., Malcolm James Ree, and Thomas R. Carretta. 1996.\n“Cognitive-Components Tests Are Not Much More Than g: An Extension\nof Kyllonen’s Analyses.” The Journal of General\nPsychology 123 (3): 193–205. https://doi.org/10.1080/00221309.1996.9921272.\n\n\nTwenge, Jean M, and Jennifer Crocker. 2002. “Race and Self-Esteem:\nMeta-Analyses Comparing Whites, Blacks, Hispanics, Asians, and American\nIndians and Comment on Gray-Little and Hafdahl (2000).”\nPsychological Bulletin 128 (3): 371–408. https://doi.org/10.1037/0033-2909.128.3.371.\n\n\nVan der Maas, Han L. J., Kees-Jan Kan, and Denny Borsboom. 2014.\n“Intelligence Is What the Intelligence Test Measures.\nSeriously.” Journal of Intelligence 2 (1): 12–15. https://doi.org/10.3390/jintelligence2010012.\n\n\nWigdor, Alexandra K. 1982. “Ability Testing: Uses, Consequences,\nand Controversies.” Educational Measurement: Issues and\nPractice 1 (3): 6–8. https://doi.org/https://doi.org/10.1111/j.1745-3992.1982.tb00659.x."
  },
  {
    "objectID": "tooabstract.html",
    "href": "tooabstract.html",
    "title": "1  Is intelligence is too abstract to measure?",
    "section": "",
    "text": "Or in other words, does the intelligence test capture all the varieties of intelligence?\nThis is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming."
  },
  {
    "objectID": "tooabstract.html#what-is-intelligence",
    "href": "tooabstract.html#what-is-intelligence",
    "title": "1  Is intelligence is too abstract to measure?",
    "section": "1.1 What is intelligence?",
    "text": "1.1 What is intelligence?\nRegarding the philosophical nature of precisely defining intelligence, “we have, no doubt, a rough and ready idea of what we mean by ‘intelligence’ and other cognate terms. The objective of scientific enquiry is to advance beyond this primitive, common-sense understanding (what is often termed ‘folk psychology’) to a more securely grounded set of scientific theories, based on empirical evidence and capable of ordering the world in possibly new and illuminating ways. We shall not achieve this goal by insisting on a rigorous, precise definition of terms at the outset. New definitions are the end product of scientific enquiry, not its starting point” Mackintosh (2011, 2)\nWe ask ourselves, “what is intelligence?” (eg. is it: processing speed; reaction time; working memory; verbal ability; spatial ability; rationality; practical intelligence; emotional intelligence etc;) Imagine a variable that predicts how someone does generally in all of these abilities. What should we call this variable?"
  },
  {
    "objectID": "tooabstract.html#the-general-factor-of-intelligence",
    "href": "tooabstract.html#the-general-factor-of-intelligence",
    "title": "1  Is intelligence is too abstract to measure?",
    "section": "1.2 The general factor of intelligence",
    "text": "1.2 The general factor of intelligence\nThe variable described above is commonly referred to as the “general factor of intelligence” or g factor. The g factor is a statistical construct that represents an individual’s overall intelligence level, and it is considered to be a fundamental concept in the field of psychology.\nThe existence of the g factor has been supported by decades of research across a wide range of intelligence tests and measures. For example, individuals who perform well on one cognitive task, such as verbal ability, tend to perform well on other cognitive tasks, such as spatial ability, indicating a positive correlation between various cognitive abilities.\nMoreover, the g factor has been found to predict a range of important life outcomes, such as academic and job performance, income level, and even health outcomes. Therefore, the g factor is not only a theoretical construct, but it also has practical implications for our understanding of human abilities and potential.\nSince nobody has ever been able to come up with an assessment for any sort of cognitive ability which does not correlate with the rest of them, (The intercorrelations are caused by a general underlying factor of intelligence, and it consistently explains 40-50% of variance in a battery of cognitive ability tests -Mackintosh (2011); Ian Deary (2001); IJ Deary (1998); Lubinski (2004)) the variable that specifically underlies all of these cognitive abilities, it would be fair to call this intelligence. In other words, while there may be many narrow abilities that people differ in, the thing that intelligence tests test for are not specifically narrow or broad abilities, but the general factor of intelligence that underlies intelligence.\n\n\n\n\nDeary, Ian. 2001. Intelligence: A Very Short Introduction. https://doi.org/10.1093/actrade/9780192893215.001.0001.\n\n\nDeary, IJ. 1998. “Differences in Mental Abilities.” BMJ (Clinical Research Ed.) 317 (7174): 1701–3. https://doi.org/10.1136/bmj.317.7174.1701.\n\n\nLubinski, David. 2004. “Introduction to the Special Section on Cognitive Abilities: 100 Years After Spearman’s (1904) \"’General Intelligence,’ Objectively Determined and Measured\".” Journal of Personality and Social Psychology 86 (1): 96–111. https://doi.org/10.1037/0022-3514.86.1.96.\n\n\nMackintosh, N. 2011. IQ and Human Intelligence. OUP Oxford. https://books.google.com/books?id=BcKcAQAAQBAJ."
  },
  {
    "objectID": "tooabstract.html#is-it-appropriate-to-define-intelligence",
    "href": "tooabstract.html#is-it-appropriate-to-define-intelligence",
    "title": "1  What is intelligence?",
    "section": "1.1 Is it appropriate to define intelligence?",
    "text": "1.1 Is it appropriate to define intelligence?\nRegarding the philosophical nature of precisely defining intelligence, “we have, no doubt, a rough and ready idea of what we mean by ‘intelligence’ and other cognate terms. The objective of scientific enquiry is to advance beyond this primitive, common-sense understanding (what is often termed ‘folk psychology’) to a more securely grounded set of scientific theories, based on empirical evidence and capable of ordering the world in possibly new and illuminating ways. We shall not achieve this goal by insisting on a rigorous, precise definition of terms at the outset. New definitions are the end product of scientific enquiry, not its starting point” (Mackintosh 2011, 2)\nWe ask ourselves, “what is intelligence?” (eg. is it: processing speed; reaction time; working memory; verbal ability; spatial ability; rationality; practical intelligence; emotional intelligence etc;) Imagine a variable that predicts how someone does generally in all of these abilities. What should we call this variable?"
  },
  {
    "objectID": "tooabstract.html#the-g-factor",
    "href": "tooabstract.html#the-g-factor",
    "title": "1  What is intelligence?",
    "section": "1.2 The g factor",
    "text": "1.2 The g factor\nThe variable described above is commonly referred to as the “general factor of intelligence” or g factor. The g factor is a statistical construct that represents an individual’s overall intelligence level, and it is considered to be a fundamental concept in the field of psychology.\nThe existence of the g factor has been supported by decades of research across a wide range of intelligence tests and measures. For example, individuals who perform well on one cognitive task, such as verbal ability, tend to perform well on other cognitive tasks, such as spatial ability, indicating a positive correlation between various cognitive abilities.\nMoreover, the g factor has been found to predict a range of important life outcomes, such as academic and job performance, income level, and even health outcomes. Therefore, the g factor is not only a theoretical construct, but it also has practical implications for our understanding of human abilities and potential.\nNobody has ever been able to come up with an assessment for any sort of cognitive ability which does not correlate with the rest of them. The intercorrelations are caused by a general underlying factor which consistently explains half of the variance in a battery of cognitive ability tests (Mackintosh 2011, 45); (Deary 2001, 222); (Deary 1998); (Lubinski 2004, 98)."
  },
  {
    "objectID": "tooabstract.html#the-cattellhorncarroll-model-of-intelligence",
    "href": "tooabstract.html#the-cattellhorncarroll-model-of-intelligence",
    "title": "1  Is intelligence is too abstract to measure?",
    "section": "1.3 The Cattell–Horn–Carroll model of intelligence",
    "text": "1.3 The Cattell–Horn–Carroll model of intelligence\nThe Cattell-Horn-Carroll (CHC) theory of intelligence is a prominent model that attempts to explain the structure of cognitive abilities. The theory proposes that intelligence is composed of a hierarchical structure of three strata: the general factor (g factor), broad abilities, and narrow abilities.\n\n\n\nCattell–Horn–Carroll theory of intelligence\n\n\nAt the top of the hierarchy is the g factor, which represents a general factor of intelligence that is common to all cognitive tasks. The broad abilities, located in the middle stratum, are a group of abilities that are less general than the g factor but still encompass a range of related cognitive tasks. Examples of broad abilities include fluid reasoning, crystallized intelligence, processing speed, and working memory. Finally, the narrow abilities, located at the bottom stratum, are specific abilities that are highly specialized and task-specific.\nThe CHC theory supports the existence of the g factor by showing that performance on different cognitive tasks is highly correlated, indicating a common underlying factor that influences performance across tasks. In addition, factor analyses of a wide range of intelligence tests consistently reveal a strong first factor that represents the g factor.\nMoreover, the CHC theory provides a framework for understanding the relationships between the g factor and other cognitive abilities. For example, fluid reasoning is considered a broad ability that is strongly related to the g factor and is involved in abstract thinking and problem-solving. Crystallized intelligence, on the other hand, is a broad ability that represents the application of knowledge and skills acquired through experience and education and is less strongly related to the g factor.\nBecause it has an impressive body of empirical support in the research literature (e.g., developmental, neurocognitive, outcome‐criterion), this model of intelligence is the one that psychologists predominantly use. It is used extensively as the foundation for selecting, organizing, and interpreting tests of intelligence and cognitive abilities. (Alfonso et al. 2005);(Beal 2006);(McGrew et al. 2005);(Schneider et al. 2012) Additionally, this model is the most comprehensive and empirically supported psychometric theory of the structure of cognitive abilities to date. (Flanagan and Dixon 2014)\nCHC is the best model from exploratory factor analysis (Carroll, B, and Press 1993) and is confirmed by confirmatory factor analysis ((Gustafsson 1984), (B. 2003), Jewsbury, Bowden, and Duff (2017)). Confirmatory factor analysis showed that purposely different IQ batteries (CAB, Hawaii Battery, WAIS, etc) were analyzed, and it turned out that the g factors computed from the respective tests were statistically indistinguishable from one another, despite the fact that the tests tapped into partly different sets of abilities. (Johnson et al. 2004), (Johnson, Nijenhuis, and Bouchard 2008)\n\n\n\n\nAlfonso, V. C., D. P. Flanagan, S. Radwan, D. P. Flanagan, and P. L. Harrison. 2005. “The Impact of the Cattell-Horn-Carroll Theory on Test Development and Interpretation of Cognitive and Academic Abilities.” In Contemporary Intellectual Assessment: Theories, Tests, and Issues, 185–202.\n\n\nB., Carroll. 2003. “The Higher-Stratum Structure of Cognitive Abilities: Current Evidence Supports g and about Ten Broad Factors.” In The Scientific Study of General Intelligence: Tribute to Arthur R. Jensen, 5–21. https://doi.org/10.1016/B978-008043793-4/50036-2.\n\n\nBeal, A Lynne. 2006. “Review of Contemporary Intellectual Assessment, Theories, Tests and Issues.”\n\n\nCarroll, J. B., C. J. B, and Cambridge University Press. 1993. Human Cognitive Abilities: A Survey of Factor-Analytic Studies. Human Cognitive Abilities: A Survey of Factor-Analytic Studies, no. 1. Cambridge University Press. https://books.google.com/books?id=jp9dt4\\_0\\_cIC.\n\n\nDeary, Ian. 1998. “Differences in Mental Abilities.” BMJ (Clinical Research Ed.) 317 (7174): 1701–3. https://doi.org/10.1136/bmj.317.7174.1701.\n\n\n———. 2001. Intelligence: A Very Short Introduction. https://doi.org/10.1093/actrade/9780192893215.001.0001.\n\n\nFlanagan, Dawn P., and Shauna G. Dixon. 2014. “The Cattell-Horn-Carroll Theory of Cognitive Abilities.” In Encyclopedia of Special Education. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118660584.ese0431.\n\n\nGustafsson, Jan-Eric. 1984. “A Unifying Model for the Structure of Intellectual Abilities.” Intelligence 8 (3): 179–203. https://doi.org/https://doi.org/10.1016/0160-2896(84)90008-4.\n\n\nJewsbury, Paul A., Stephen C. Bowden, and Kevin Duff. 2017. “The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment.” Journal of Psychoeducational Assessment 35 (6): 547–67. https://doi.org/10.1177/0734282916651360.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nLubinski, David. 2004. “Introduction to the Special Section on Cognitive Abilities: 100 Years After Spearman’s (1904) \"’General Intelligence,’ Objectively Determined and Measured\".” Journal of Personality and Social Psychology 86 (1): 96–111. https://doi.org/10.1037/0022-3514.86.1.96.\n\n\nMackintosh, N. 2011. IQ and Human Intelligence. OUP Oxford. https://books.google.com/books?id=BcKcAQAAQBAJ.\n\n\nMcGrew, Kevin S, DP Flanagan, JL Genshaft, and PL Harrison. 2005. Contemporary Intellectual Assessment: Theories, Tests, and Issues. The Guilford Press New York, NY, USA:\n\n\nSchneider, W Joel, Kevin S McGrew, DP Flanagan, and PL Harrison. 2012. “Contemporary Intellectual Assessment: Theories, Tests, and Issues.” Institute for Applied Psychometrics (IAP)."
  },
  {
    "objectID": "tooabstract.html#the-chc-model",
    "href": "tooabstract.html#the-chc-model",
    "title": "1  What is intelligence?",
    "section": "1.3 The CHC model",
    "text": "1.3 The CHC model\nThe Cattell-Horn-Carroll (CHC) theory of intelligence is a prominent model that attempts to explain the structure of cognitive abilities. The theory proposes that intelligence is composed of a hierarchical structure of three strata: the general factor (g factor), broad abilities, and narrow abilities.\n\n\n\nCattell–Horn–Carroll theory of intelligence\n\n\nAt the top of the hierarchy is the g factor, which represents a general factor of intelligence that is common to all cognitive tasks. The broad abilities, located in the middle stratum, are a group of abilities that are less general than the g factor but still encompass a range of related cognitive tasks. Examples of broad abilities include fluid reasoning, crystallized intelligence, processing speed, and working memory. Finally, the narrow abilities, located at the bottom stratum, are specific abilities that are highly specialized and task-specific.\nThe CHC theory supports the existence of the g factor by showing that performance on different cognitive tasks is highly correlated, indicating a common underlying factor that influences performance across tasks. In addition, factor analyses of a wide range of intelligence tests consistently reveal a strong first factor that represents the g factor.\nMoreover, the CHC theory provides a framework for understanding the relationships between the g factor and other cognitive abilities. For example, fluid reasoning is considered a broad ability that is strongly related to the g factor and is involved in abstract thinking and problem-solving. Crystallized intelligence, on the other hand, is a broad ability that represents the application of knowledge and skills acquired through experience and education and is less strongly related to the g factor.\nBecause it has an impressive body of empirical support in the research literature (e.g., developmental, neurocognitive, outcome‐criterion), this model of intelligence is the one that psychologists predominantly use. It is used extensively as the foundation for selecting, organizing, and interpreting tests of intelligence and cognitive abilities. (Alfonso et al. 2005);(Beal 2006);(McGrew et al. 2005);(Schneider et al. 2012) Additionally, this model is the most comprehensive and empirically supported psychometric theory of the structure of cognitive abilities to date. (Flanagan and Dixon 2014)\nCHC is the best model from exploratory factor analysis (J. B. Carroll, B, and Press 1993) and is confirmed by confirmatory factor analysis (Gustafsson 1984); (J. Carroll 2003) (Jewsbury, Bowden, and Duff 2017). Confirmatory factor analysis showed that purposely different IQ batteries (CAB, Hawaii Battery, WAIS, etc) were analyzed, and it turned out that the g factors computed from the respective tests were statistically indistinguishable from one another, despite the fact that the tests tapped into partly different sets of abilities. (Johnson et al. 2004); (Johnson, Nijenhuis, and Bouchard 2008)\n\n\n\n\nAlfonso, V. C., D. P. Flanagan, S. Radwan, D. P. Flanagan, and P. L. Harrison. 2005. “The Impact of the Cattell-Horn-Carroll Theory on Test Development and Interpretation of Cognitive and Academic Abilities.” In Contemporary Intellectual Assessment: Theories, Tests, and Issues, 185–202.\n\n\nBeal, A Lynne. 2006. “Review of Contemporary Intellectual Assessment, Theories, Tests and Issues.”\n\n\nCarroll, J. B., C. J. B, and Cambridge University Press. 1993. Human Cognitive Abilities: A Survey of Factor-Analytic Studies. Human Cognitive Abilities: A Survey of Factor-Analytic Studies, no. 1. Cambridge University Press. https://books.google.com/books?id=jp9dt4\\_0\\_cIC.\n\n\nCarroll, John. 2003. “The Higher-Stratum Structure of Cognitive Abilities: Current Evidence Supports g and about Ten Broad Factors.” In The Scientific Study of General Intelligence: Tribute to Arthur R. Jensen, 5–21. https://doi.org/10.1016/B978-008043793-4/50036-2.\n\n\nDeary, Ian. 1998. “Differences in Mental Abilities.” BMJ (Clinical Research Ed.) 317 (7174): 1701–3. https://doi.org/10.1136/bmj.317.7174.1701.\n\n\n———. 2001. Intelligence: A Very Short Introduction. https://doi.org/10.1093/actrade/9780192893215.001.0001.\n\n\nFlanagan, Dawn P., and Shauna G. Dixon. 2014. “The Cattell-Horn-Carroll Theory of Cognitive Abilities.” In Encyclopedia of Special Education. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118660584.ese0431.\n\n\nGustafsson, Jan-Eric. 1984. “A Unifying Model for the Structure of Intellectual Abilities.” Intelligence 8 (3): 179–203. https://doi.org/https://doi.org/10.1016/0160-2896(84)90008-4.\n\n\nJewsbury, Paul, Stephen Bowden, and Kevin Duff. 2017. “The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment.” Journal of Psychoeducational Assessment 35 (6): 547–67. https://doi.org/10.1177/0734282916651360.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nLubinski, David. 2004. “Introduction to the Special Section on Cognitive Abilities: 100 Years After Spearman’s (1904) \"’General Intelligence,’ Objectively Determined and Measured\".” Journal of Personality and Social Psychology 86 (1): 96–111. https://doi.org/10.1037/0022-3514.86.1.96.\n\n\nMackintosh, Nicholas. 2011. IQ and Human Intelligence. OUP Oxford. https://books.google.com/books?id=BcKcAQAAQBAJ.\n\n\nMcGrew, Kevin S, DP Flanagan, JL Genshaft, and PL Harrison. 2005. Contemporary Intellectual Assessment: Theories, Tests, and Issues. The Guilford Press New York, NY, USA:\n\n\nSchneider, W Joel, Kevin S McGrew, DP Flanagan, and PL Harrison. 2012. “Contemporary Intellectual Assessment: Theories, Tests, and Issues.” Institute for Applied Psychometrics (IAP)."
  },
  {
    "objectID": "justtests.html#the-positive-manifold",
    "href": "justtests.html#the-positive-manifold",
    "title": "2  What do intelligence tests measure?",
    "section": "2.1 The Positive Manifold",
    "text": "2.1 The Positive Manifold\nThe positive manifold refers to the observation that all intelligence subtests, including scholastic and social intelligence tests, correlate positively. This pattern indicates that intelligence can be measured as a composite score derived from various cognitive abilities.\nThe consistency of this positive correlation, known as the g factor, has been observed even in tests initially designed to measure other traits or abilities. Since it is so easy for cognitive batteries to measure intelligence, some tests that were created with the intention of measuring more narrow abilities end up unintentionally measuring the g-factor. For example, the Cognitive Assessment System (CAS) and the Cognitive Abilities Measurement (CAM) battery were both designed to measure cognitive processes – not g, but they still measure g anyway (Keith, Kranzler, and Flanagan 2001); (Stauffer, Ree, and Carretta 1996). Confirmatory factor analysis showed that different cognitive batteries (CAB, Hawaii Battery, WAIS) were analyzed, and it turned out that the g factors computed from the three tests (five tests in Johnson 2008) were statistically indistinguishable from one another, despite the fact that the tests tapped into partly different sets of abilities (Johnson et al. 2004); (Johnson, Nijenhuis, and Bouchard 2008)."
  },
  {
    "objectID": "justtests.html#are-intelligence-tests-statistically-valid",
    "href": "justtests.html#are-intelligence-tests-statistically-valid",
    "title": "2  What do intelligence tests measure?",
    "section": "2.3 Are intelligence tests statistically valid?",
    "text": "2.3 Are intelligence tests statistically valid?\nTo be added: test validity- too much to add rn\n\n2.3.1 Wechsler Adult Intelligence Scale (WAIS)\n\n\n2.3.2 Wechsler Intelligence Scale for Children (WISC)\n\n\n2.3.3 Stanford–Binet Intelligence Scales, Fifth Edition (SB-V)\n\n\n2.3.4 Woodcock–Johnson Tests of Cognitive Abilities\n\n\n\n\nCanivez, G. L. 2017. “Test Review of Woodcock-Johnson® IV.” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nCanivez, Gary, and Marley Watkins. 2016. “Review of the Wechsler Intelligence Scale for Children–Fifth Edition: Critique, Commentary, and Independent Analyses.” In, 683–702.\n\n\nClimie, Emma A., and Kristin Rostad. 2011. “Test Review: Wechsler Adult Intelligence Scale.” Journal of Psychoeducational Assessment 29 (6): 581–86. https://doi.org/10.1177/0734282911408707.\n\n\nCormier, Damien C., Kathleen E. Kennedy, and Alexandra M. Aquilina. 2016. “Test Review: Wechsler Intelligence Scale for Children, Fifth Edition: Canadian 322 (WISC-VCDN) by d. Wechsler.” Canadian Journal of School Psychology 31 (4): 322–34. https://doi.org/10.1177/0829573516648941.\n\n\nDetterman, Douglas K. 2014. “You Should Be Teaching Intelligence!” Intelligence 42: 148–51. https://doi.org/https://doi.org/10.1016/j.intell.2013.07.021.\n\n\nJanzen, Henry L., John E. Obrzut, and Christopher W. Marusiak. 2004. “Test Review: Roid, g. H. (2003). Stanford-Binet Intelligence Scales, Fifth Edition (SB:v). Itasca, IL: Riverside Publishing.” Canadian Journal of School Psychology 19 (1-2): 235–44. https://doi.org/10.1177/082957350401900113.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nKeith, Timothy, John Kranzler, and Dawn Flanagan. 2001. “What Does the Cognitive Assessment System (CAS) Measure? Joint Confirmatory Factor Analysis of the CAS and the Woodcock-Johnson Tests of Cognitive Ability.” School Psychology Review 30 (March): 89–119. https://doi.org/10.1080/02796015.2001.12086102.\n\n\nMadle, R. A. 2017. “[Test Review of Woodcock-Johnson® IV].” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nStauffer, Joseph M., Malcolm James Ree, and Thomas R. Carretta. 1996. “Cognitive-Components Tests Are Not Much More Than g: An Extension of Kyllonen’s Analyses.” The Journal of General Psychology 123 (3): 193–205. https://doi.org/10.1080/00221309.1996.9921272.\n\n\nVan der Maas, Han L. J., Kees-Jan Kan, and Denny Borsboom. 2014. “Intelligence Is What the Intelligence Test Measures. Seriously.” Journal of Intelligence 2 (1): 12–15. https://doi.org/10.3390/jintelligence2010012."
  },
  {
    "objectID": "justtests.html#are-intelligence-tests-statistically-valid-1",
    "href": "justtests.html#are-intelligence-tests-statistically-valid-1",
    "title": "2  What do intelligence tests measure?",
    "section": "2.3 Are intelligence tests statistically valid?",
    "text": "2.3 Are intelligence tests statistically valid?\nMillions of intelligence tests are given in educational and organizational settings and also serve as clinical tools. Intelligence tests are among the most reliable and valid of all psychological tests and assessments. Detterman (2014) Intelligence tests are sound in terms of both validity and reliability.\nConcurrent validity: The WCJIV correlated with FSIQ, WAIS, WBIS, among other tests all at 0.7 or higher. (Canivez 2017) The FSIQ correlated with WAIS, SBIV, WJIII etc at 0.78 or higher. (Janzen et al, 2004). External validity: Geneticists have identified specific genetic variances correlated with neurodevelopment and brain functioning that are arises from a distributed network of well-connected brain regions, mostly located in the parietal and frontal lobes (Deary et al., 2010; Haier, 2017; Jung & Haier, 2007; Penke et al., 2012). In turn, these regions—and the brain as a whole—seem to be larger, healthier, and more organized in brighter people (Flashman et al., 1997; Genç et al., 2018; Gignac & Bates, 2017; W. D. Hill et al., 2019). Geneticists have identified specific genetic variances correlated with neurodevelopment and brain functioning that are also associated with interindividual differences in intelligence and/or educational attainment (e.g., Adams et al., 2016; Lee et al., 2018; Okbay et al., 2016). Predictive Validity: Even though Intelligence tests are not designed to correlate with any real world behavior, they end up being some the best predictors for educational, occupational, and financial attainment. IQ scores highly predict the dispositional capabilities of people. (Strenze 2007, 2015; Kaufman 2009) Construct Validity: The WAIS-IV, WJ-III, and SB-V all are construct valid. (Wechsler, 2008; Janzen et al, 2004 Woodcock et al, 2001) Using neuroscientific Test-retest reliability: First, from 6 years old onward, correlations among IQ scores range from moderately strong to strong, suggesting that IQ scores are relatively stable once children reach school age (Deary, 2014; Hunt, 2010; Sternberg, Grigorenko, & Bundy, 2001). Ye et al, 2018 records the use confirmatory factor analysis at different developmental stages to confirm the stability of intelligence throughout adulthood. Split-half reliability: Abad, Quiroga, and Colom (2017) records that the reliability coefficient of most intelligence tests is 0.97 or higher. For context, “if the reliability of a standardized test is above .80, it is said to have very good reliability”\n\n\n\n\nCanivez, G. L. 2017. “Test Review of Woodcock-Johnson® IV.” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nCanivez, Gary, and Marley Watkins. 2016. “Review of the Wechsler Intelligence Scale for Children–Fifth Edition: Critique, Commentary, and Independent Analyses.” In, 683–702.\n\n\nClimie, Emma A., and Kristin Rostad. 2011. “Test Review: Wechsler Adult Intelligence Scale.” Journal of Psychoeducational Assessment 29 (6): 581–86. https://doi.org/10.1177/0734282911408707.\n\n\nCormier, Damien C., Kathleen E. Kennedy, and Alexandra M. Aquilina. 2016. “Test Review: Wechsler Intelligence Scale for Children, Fifth Edition: Canadian 322 (WISC-VCDN) by d. Wechsler.” Canadian Journal of School Psychology 31 (4): 322–34. https://doi.org/10.1177/0829573516648941.\n\n\nDetterman, Douglas K. 2014. “You Should Be Teaching Intelligence!” Intelligence 42: 148–51. https://doi.org/https://doi.org/10.1016/j.intell.2013.07.021.\n\n\nJanzen, Henry L., John E. Obrzut, and Christopher W. Marusiak. 2004. “Test Review: Roid, g. H. (2003). Stanford-Binet Intelligence Scales, Fifth Edition (SB:v). Itasca, IL: Riverside Publishing.” Canadian Journal of School Psychology 19 (1-2): 235–44. https://doi.org/10.1177/082957350401900113.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nKeith, Timothy, John Kranzler, and Dawn Flanagan. 2001. “What Does the Cognitive Assessment System (CAS) Measure? Joint Confirmatory Factor Analysis of the CAS and the Woodcock-Johnson Tests of Cognitive Ability.” School Psychology Review 30 (March): 89–119. https://doi.org/10.1080/02796015.2001.12086102.\n\n\nMadle, R. A. 2017. “[Test Review of Woodcock-Johnson® IV].” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nStauffer, Joseph M., Malcolm James Ree, and Thomas R. Carretta. 1996. “Cognitive-Components Tests Are Not Much More Than g: An Extension of Kyllonen’s Analyses.” The Journal of General Psychology 123 (3): 193–205. https://doi.org/10.1080/00221309.1996.9921272.\n\n\nVan der Maas, Han L. J., Kees-Jan Kan, and Denny Borsboom. 2014. “Intelligence Is What the Intelligence Test Measures. Seriously.” Journal of Intelligence 2 (1): 12–15. https://doi.org/10.3390/jintelligence2010012."
  },
  {
    "objectID": "justtests.html#are-intelligence-tests-reliable",
    "href": "justtests.html#are-intelligence-tests-reliable",
    "title": "2  What do intelligence tests measure?",
    "section": "2.2 Are intelligence tests reliable?",
    "text": "2.2 Are intelligence tests reliable?\nIntelligence tests are among the most reliable and valid psychological tests and assessments and as such, they are used extensively in educational, organizational, and clinical settings (Detterman 2014).\nAccording to the Wikipedia page on IQ:\nThe most commonly used individual IQ test series is the Wechsler Adult Intelligence Scale (WAIS) for adults and the Wechsler Intelligence Scale for Children (WISC) for school-age test-takers. Other commonly used individual IQ tests (some of which do not label their standard scores as “IQ” scores) include the current versions of the Stanford–Binet Intelligence Scales, Woodcock–Johnson Tests of Cognitive Abilities…\nFor brevity, I will only focus on the first four tests mentioned: WAIS, WISC, SB5, and WJ-IV.\n\n2.2.1 Wechsler Adult Intelligence Scale (WAIS)\nAccording to a review by Climie and Rostad (2011)\n\nInternal consistency. The WAIS-IV’s reliability was measured using split-half and Cronbach’s coefficient alpha. All reported values were based on U.S. norms. The average reliability coefficients for the subtest scores ranged from acceptable (.78) to excellent (≥.90), while the four composite scores had excellent reliability coefficients (all ≥.90), with the FSIQ reporting a reliability coefficient of .98.\nTest-retest reliability. Test-retest reliabilities were obtained using Pearson’s product-moment correlation for four age bands (16-29, 30-54, 55-69, and 70-90), with time intervals between testing ranging from 8 to 82 days and a mean of 22 days. Overall, the WAIS-IV has acceptable stability across time for each of the four age bands. Subtest stability coefficients range from adequate (.74) to excellent (.90), with a majority of scores falling within the good range (.80s). Composite scores ranged from .87 to .96, with the FSIQ stability coefficient reaching an excellent value of .96.\nInterrater reliability. To ascertain acceptable levels of interrater reliability, all WAIS-IV protocols were scored by two independent scorers. General interrater agreement was high (.98 to .99), and for the verbal subtests, raters obtained excellent levels of reliability, ranging from .91 to .97.\n\n\n\n2.2.2 Wechsler Intelligence Scale for Children (WISC)\nAccording to a review by Cormier, Kennedy, and Aquilina (2016)\n\nThe formula used to calculate internal consistency reliability was recommended by Guilford (1954), Haertel (2006), and Nunnally and Bernstein (1994).\nFisher’s z transformation was used to calculate the average reliability coefficients.\nInternal consistency reliability for composite, subtest, and process scores ranges from r = .80 to r = .96.\nThe split-half method was used to calculate the reliability coefficients for all subtests, except for Coding, Symbol Search, Cancellation, Naming Speed Literacy, Naming Speed Quantity, Immediate Symbol Translation, or Delayed Symbol Translation subtests.\nThe average reliability coefficient for FSIQ is r = .96.\nThe average reliability coefficients for the primary and secondary subtests range from r = .81 to r = .94.\nThe process scores reliability coefficients range from r = .80 to r = .85.\nThe overall reliability coefficients for the primary index scores range from r = .88 to r = .93, and for the ancillary index scores range from r = .92 to r = .95.\nTest-retest reliability coefficients ranged from r = .71 to r = .94, with a mean test-retest interval of 26 days.\nEvidence of inter-scorer agreement was obtained, with the range of inter-scorer agreement being r = .98 to r = .99.\nThe average of the reliability coefficients for primary and secondary subtests from the WISC-VCDN ranges from good to excellent.\nThe reliability estimates for the WISC-VCDN scores demonstrate strong precision, which contributes to minimizing the effects of error in the measurement of the targeted abilities.\n\nAccording to a review by G. Canivez and Watkins (2016)\n\nReliability estimates of WISC-V scores reported in the WISC-V Technical and Interpretive Manual were derived using three methods: internal consistency, test-retest (stability), and interscorer agreement.\nInternal consistency was measured through the use of split-half and Cronbach’s coefficient alpha, with data provided for each of the 15 core and supplemental subtests and five composite scores (including the FSIQ).\nTest-retest reliabilities were obtained through repeated administration of the WAIS-IV, with time intervals between testing ranging from 8 to 82 days and a mean of 22 days.\nTo ascertain acceptable levels of interrater reliability, all WAIS-IV protocols were scored by two independent scorers.\nAverage coefficients across the 11 age groups for the composite scores ranged from .88 to .96 and were higher than those obtained for subtests and process scores.\nInternal consistency estimates for the primary and secondary subtests ranged from .81 to .94 while process scores ranged from .80 to .88.\nReliability estimates for the complementary subtests, process, and composite scores are provided.\nStandard errors of measurement based on reliability coefficients are presented in Table 4.4 of the WISC-V Technical and Interpretive Manual.\nShort-term test-retest stability estimates were provided for WISC-V scores where the WISC-V was twice administered to a sample of 218 children.\nInterscorer agreement ranged from .97 to .99.\n\n\n\n2.2.3 Stanford–Binet Intelligence Scales, Fifth Edition (SB-V)\nAccording to a review by Janzen, Obrzut, and Marusiak (2004)\n\nThe internal consistency (split-half reliability) for the Full Scale IQ (FSIQ), Nonverbal IQ (NVIQ), and Verbal IQ (VIQ) ranges between .95 and .98; for the five Factor Indices, it is between .90 and .92; and, for the ten subtests, it ranges between .84 and .89.\nIQ scores on the SB:V appear to be quite stable across time and less affected by practice effects.\nThe median inter-scorer correlation is .90, and correlations on multiple-point responses (0, 1, 2) range from .74 to .97.\n\n\n\n2.2.4 Woodcock–Johnson Tests of Cognitive Abilities\nAccording to a review by G. L. Canivez (2017)\n\nInternal consistency reliabilities for untimed tests and dichotomously scored items were estimated with the split-half method, whereas tests with subtests and cluster reliabilities were estimated with Mosier’s (1943) formula for unweighted composites.\nReliability for speeded tests was estimated using the test-retest method with a 1-day retest interval, and correlations were corrected for range restriction.\nMedian reliability coefficients were uniformly high: 38 of 39 were .80 or higher, and 17 were .90 or higher. Test-retest correlations for speeded tests were mostly in the .80 to .90 range.\nCluster scores include two or more tests and as such produce higher reliability estimates as predicted by true score theory. The WJ IV technical manual recommended using cluster scores in interpretation, especially when used in individual decision-making.\nModel-based reliability coefficients (omega hierarchical) could be assessed to determine unique true score variance captured by the different scores.\nAlternate forms equivalence was examined and item difficulty and content were found to be similar among the forms.\n\nAccording to Madle (2017)\n\nMedian internal consistency coefficient for WJ IV COG GIA score is .97, with median coefficients of .95 to .97 for different age groups\nGf-Gc reliability coefficients range from .94 to .98 (median = .96)\nBIA reliability coefficient median is .94 (.92-.95)\nMedian coefficients for the various cognitive clusters range from .86 (Visual Processing) to .91 (Short-Term Working Memory)\nOral Language reliability coefficients range from .89 for Oral Expression to .92 for Broad Oral Language\nAcademic cluster median reliability coefficients range from .92 to .97, with a Broad Achievement score median reliability coefficient of .99\nTest-retest studies were completed for speeded tests only, and median reliability coefficients ranged from .88 to .92 for three different age groups\nEquivalence of item difficulties and item content for the achievement forms was examined and found to be strong."
  },
  {
    "objectID": "2justtests.html#the-positive-manifold",
    "href": "2justtests.html#the-positive-manifold",
    "title": "2  What do intelligence tests measure?",
    "section": "2.1 The Positive Manifold",
    "text": "2.1 The Positive Manifold\nThe positive manifold refers to the observation that all intelligence subtests, including scholastic and social intelligence tests, correlate positively. This pattern indicates that intelligence can be measured as a composite score derived from various cognitive abilities.\nThe consistency of this positive correlation, known as the g factor, has been observed even in tests initially designed to measure other traits or abilities. Since it is so easy for cognitive batteries to measure intelligence, some tests that were created with the intention of measuring more narrow abilities end up unintentionally measuring the g-factor. For example, the Cognitive Assessment System (CAS) and the Cognitive Abilities Measurement (CAM) battery were both designed to measure cognitive processes – not g, but they still measure g anyway (Keith, Kranzler, and Flanagan 2001); (Stauffer, Ree, and Carretta 1996). Confirmatory factor analysis showed that different cognitive batteries (CAB, Hawaii Battery, WAIS) were analyzed, and it turned out that the g factors computed from the three tests (five tests in Johnson 2008) were statistically indistinguishable from one another, despite the fact that the tests tapped into partly different sets of abilities (Johnson et al. 2004); (Johnson, Nijenhuis, and Bouchard 2008)."
  },
  {
    "objectID": "2justtests.html#are-intelligence-tests-reliable",
    "href": "2justtests.html#are-intelligence-tests-reliable",
    "title": "2  What do intelligence tests measure?",
    "section": "2.2 Are intelligence tests reliable?",
    "text": "2.2 Are intelligence tests reliable?\nIntelligence tests are among the most reliable and valid psychological tests and assessments and as such, they are used extensively in educational, organizational, and clinical settings (Detterman 2014).\nAccording to the Wikipedia page on IQ:\nThe most commonly used individual IQ test series is the Wechsler Adult Intelligence Scale (WAIS) for adults and the Wechsler Intelligence Scale for Children (WISC) for school-age test-takers. Other commonly used individual IQ tests (some of which do not label their standard scores as “IQ” scores) include the current versions of the Stanford–Binet Intelligence Scales, Woodcock–Johnson Tests of Cognitive Abilities…\nFor brevity, I will only focus on the first four tests mentioned: WAIS, WISC, SB5, and WJ-IV.\n\n2.2.1 Wechsler Adult Intelligence Scale (WAIS)\nAccording to a review by Climie and Rostad (2011)\n\nInternal consistency. The WAIS-IV’s reliability was measured using split-half and Cronbach’s coefficient alpha. All reported values were based on U.S. norms. The average reliability coefficients for the subtest scores ranged from acceptable (.78) to excellent (≥.90), while the four composite scores had excellent reliability coefficients (all ≥.90), with the FSIQ reporting a reliability coefficient of .98.\nTest-retest reliability. Test-retest reliabilities were obtained using Pearson’s product-moment correlation for four age bands (16-29, 30-54, 55-69, and 70-90), with time intervals between testing ranging from 8 to 82 days and a mean of 22 days. Overall, the WAIS-IV has acceptable stability across time for each of the four age bands. Subtest stability coefficients range from adequate (.74) to excellent (.90), with a majority of scores falling within the good range (.80s). Composite scores ranged from .87 to .96, with the FSIQ stability coefficient reaching an excellent value of .96.\nInterrater reliability. To ascertain acceptable levels of interrater reliability, all WAIS-IV protocols were scored by two independent scorers. General interrater agreement was high (.98 to .99), and for the verbal subtests, raters obtained excellent levels of reliability, ranging from .91 to .97.\n\n\n\n2.2.2 Wechsler Intelligence Scale for Children (WISC)\nAccording to a review by Cormier, Kennedy, and Aquilina (2016)\n\nThe formula used to calculate internal consistency reliability was recommended by Guilford (1954), Haertel (2006), and Nunnally and Bernstein (1994).\nFisher’s z transformation was used to calculate the average reliability coefficients.\nInternal consistency reliability for composite, subtest, and process scores ranges from r = .80 to r = .96.\nThe split-half method was used to calculate the reliability coefficients for all subtests, except for Coding, Symbol Search, Cancellation, Naming Speed Literacy, Naming Speed Quantity, Immediate Symbol Translation, or Delayed Symbol Translation subtests.\nThe average reliability coefficient for FSIQ is r = .96.\nThe average reliability coefficients for the primary and secondary subtests range from r = .81 to r = .94.\nThe process scores reliability coefficients range from r = .80 to r = .85.\nThe overall reliability coefficients for the primary index scores range from r = .88 to r = .93, and for the ancillary index scores range from r = .92 to r = .95.\nTest-retest reliability coefficients ranged from r = .71 to r = .94, with a mean test-retest interval of 26 days.\nEvidence of inter-scorer agreement was obtained, with the range of inter-scorer agreement being r = .98 to r = .99.\nThe average of the reliability coefficients for primary and secondary subtests from the WISC-VCDN ranges from good to excellent.\nThe reliability estimates for the WISC-VCDN scores demonstrate strong precision, which contributes to minimizing the effects of error in the measurement of the targeted abilities.\n\nAccording to a review by G. Canivez and Watkins (2016)\n\nReliability estimates of WISC-V scores reported in the WISC-V Technical and Interpretive Manual were derived using three methods: internal consistency, test-retest (stability), and interscorer agreement.\nInternal consistency was measured through the use of split-half and Cronbach’s coefficient alpha, with data provided for each of the 15 core and supplemental subtests and five composite scores (including the FSIQ).\nTest-retest reliabilities were obtained through repeated administration of the WAIS-IV, with time intervals between testing ranging from 8 to 82 days and a mean of 22 days.\nTo ascertain acceptable levels of interrater reliability, all WAIS-IV protocols were scored by two independent scorers.\nAverage coefficients across the 11 age groups for the composite scores ranged from .88 to .96 and were higher than those obtained for subtests and process scores.\nInternal consistency estimates for the primary and secondary subtests ranged from .81 to .94 while process scores ranged from .80 to .88.\nReliability estimates for the complementary subtests, process, and composite scores are provided.\nStandard errors of measurement based on reliability coefficients are presented in Table 4.4 of the WISC-V Technical and Interpretive Manual.\nShort-term test-retest stability estimates were provided for WISC-V scores where the WISC-V was twice administered to a sample of 218 children.\nInterscorer agreement ranged from .97 to .99.\n\n\n\n2.2.3 Stanford–Binet Intelligence Scales, Fifth Edition (SB-V)\nAccording to a review by Janzen, Obrzut, and Marusiak (2004)\n\nThe internal consistency (split-half reliability) for the Full Scale IQ (FSIQ), Nonverbal IQ (NVIQ), and Verbal IQ (VIQ) ranges between .95 and .98; for the five Factor Indices, it is between .90 and .92; and, for the ten subtests, it ranges between .84 and .89.\nIQ scores on the SB:V appear to be quite stable across time and less affected by practice effects.\nThe median inter-scorer correlation is .90, and correlations on multiple-point responses (0, 1, 2) range from .74 to .97.\n\n\n\n2.2.4 Woodcock–Johnson Tests of Cognitive Abilities\nAccording to a review by G. L. Canivez (2017)\n\nInternal consistency reliabilities for untimed tests and dichotomously scored items were estimated with the split-half method, whereas tests with subtests and cluster reliabilities were estimated with Mosier’s (1943) formula for unweighted composites.\nReliability for speeded tests was estimated using the test-retest method with a 1-day retest interval, and correlations were corrected for range restriction.\nMedian reliability coefficients were uniformly high: 38 of 39 were .80 or higher, and 17 were .90 or higher. Test-retest correlations for speeded tests were mostly in the .80 to .90 range.\nCluster scores include two or more tests and as such produce higher reliability estimates as predicted by true score theory. The WJ IV technical manual recommended using cluster scores in interpretation, especially when used in individual decision-making.\nModel-based reliability coefficients (omega hierarchical) could be assessed to determine unique true score variance captured by the different scores.\nAlternate forms equivalence was examined and item difficulty and content were found to be similar among the forms.\n\nAccording to Madle (2017)\n\nMedian internal consistency coefficient for WJ IV COG GIA score is .97, with median coefficients of .95 to .97 for different age groups\nGf-Gc reliability coefficients range from .94 to .98 (median = .96)\nBIA reliability coefficient median is .94 (.92-.95)\nMedian coefficients for the various cognitive clusters range from .86 (Visual Processing) to .91 (Short-Term Working Memory)\nOral Language reliability coefficients range from .89 for Oral Expression to .92 for Broad Oral Language\nAcademic cluster median reliability coefficients range from .92 to .97, with a Broad Achievement score median reliability coefficient of .99\nTest-retest studies were completed for speeded tests only, and median reliability coefficients ranged from .88 to .92 for three different age groups\nEquivalence of item difficulties and item content for the achievement forms was examined and found to be strong."
  },
  {
    "objectID": "2justtests.html#are-intelligence-tests-statistically-valid",
    "href": "2justtests.html#are-intelligence-tests-statistically-valid",
    "title": "2  What do intelligence tests measure?",
    "section": "2.3 Are intelligence tests statistically valid?",
    "text": "2.3 Are intelligence tests statistically valid?\nTo be added: test validity- too much to add rn\n\n2.3.1 Wechsler Adult Intelligence Scale (WAIS)\n\n\n2.3.2 Wechsler Intelligence Scale for Children (WISC)\n\n\n2.3.3 Stanford–Binet Intelligence Scales, Fifth Edition (SB-V)\n\n\n2.3.4 Woodcock–Johnson Tests of Cognitive Abilities\n\n\n\n\nCanivez, G. L. 2017. “Test Review of Woodcock-Johnson® IV.” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nCanivez, Gary, and Marley Watkins. 2016. “Review of the Wechsler Intelligence Scale for Children–Fifth Edition: Critique, Commentary, and Independent Analyses.” In, 683–702.\n\n\nClimie, Emma A., and Kristin Rostad. 2011. “Test Review: Wechsler Adult Intelligence Scale.” Journal of Psychoeducational Assessment 29 (6): 581–86. https://doi.org/10.1177/0734282911408707.\n\n\nCormier, Damien C., Kathleen E. Kennedy, and Alexandra M. Aquilina. 2016. “Test Review: Wechsler Intelligence Scale for Children, Fifth Edition: Canadian 322 (WISC-VCDN) by d. Wechsler.” Canadian Journal of School Psychology 31 (4): 322–34. https://doi.org/10.1177/0829573516648941.\n\n\nDetterman, Douglas K. 2014. “You Should Be Teaching Intelligence!” Intelligence 42: 148–51. https://doi.org/https://doi.org/10.1016/j.intell.2013.07.021.\n\n\nJanzen, Henry L., John E. Obrzut, and Christopher W. Marusiak. 2004. “Test Review: Roid, g. H. (2003). Stanford-Binet Intelligence Scales, Fifth Edition (SB:v). Itasca, IL: Riverside Publishing.” Canadian Journal of School Psychology 19 (1-2): 235–44. https://doi.org/10.1177/082957350401900113.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nKeith, Timothy, John Kranzler, and Dawn Flanagan. 2001. “What Does the Cognitive Assessment System (CAS) Measure? Joint Confirmatory Factor Analysis of the CAS and the Woodcock-Johnson Tests of Cognitive Ability.” School Psychology Review 30 (March): 89–119. https://doi.org/10.1080/02796015.2001.12086102.\n\n\nMadle, R. A. 2017. “[Test Review of Woodcock-Johnson® IV].” In The Twentieth Mental Measurements Yearbook, edited by J. F. Carlson, K. F. Geisinger, and J. L. Jonson.\n\n\nStauffer, Joseph M., Malcolm James Ree, and Thomas R. Carretta. 1996. “Cognitive-Components Tests Are Not Much More Than g: An Extension of Kyllonen’s Analyses.” The Journal of General Psychology 123 (3): 193–205. https://doi.org/10.1080/00221309.1996.9921272.\n\n\nVan der Maas, Han L. J., Kees-Jan Kan, and Denny Borsboom. 2014. “Intelligence Is What the Intelligence Test Measures. Seriously.” Journal of Intelligence 2 (1): 12–15. https://doi.org/10.3390/jintelligence2010012."
  },
  {
    "objectID": "1tooabstract.html#is-it-appropriate-to-define-intelligence",
    "href": "1tooabstract.html#is-it-appropriate-to-define-intelligence",
    "title": "1  What is intelligence?",
    "section": "1.1 Is it appropriate to define intelligence?",
    "text": "1.1 Is it appropriate to define intelligence?\nRegarding the philosophical nature of precisely defining intelligence, “we have, no doubt, a rough and ready idea of what we mean by ‘intelligence’ and other cognate terms. The objective of scientific enquiry is to advance beyond this primitive, common-sense understanding (what is often termed ‘folk psychology’) to a more securely grounded set of scientific theories, based on empirical evidence and capable of ordering the world in possibly new and illuminating ways. We shall not achieve this goal by insisting on a rigorous, precise definition of terms at the outset. New definitions are the end product of scientific enquiry, not its starting point” (Mackintosh 2011, 2)\nWe ask ourselves, “what is intelligence?” (eg. is it: processing speed; reaction time; working memory; verbal ability; spatial ability; rationality; practical intelligence; emotional intelligence etc;) Imagine a variable that predicts how someone does generally in all of these abilities. What should we call this variable?"
  },
  {
    "objectID": "1tooabstract.html#the-g-factor",
    "href": "1tooabstract.html#the-g-factor",
    "title": "1  What is intelligence?",
    "section": "1.2 The g factor",
    "text": "1.2 The g factor\nThe variable described above is commonly referred to as the “general factor of intelligence” or g factor. The g factor is a statistical construct that represents an individual’s overall intelligence level, and it is considered to be a fundamental concept in the field of psychology.\nThe existence of the g factor has been supported by decades of research across a wide range of intelligence tests and measures. For example, individuals who perform well on one cognitive task, such as verbal ability, tend to perform well on other cognitive tasks, such as spatial ability, indicating a positive correlation between various cognitive abilities.\nMoreover, the g factor has been found to predict a range of important life outcomes, such as academic and job performance, income level, and even health outcomes. Therefore, the g factor is not only a theoretical construct, but it also has practical implications for our understanding of human abilities and potential.\nNobody has ever been able to come up with an assessment for any sort of cognitive ability which does not correlate with the rest of them. The intercorrelations are caused by a general underlying factor which consistently explains half of the variance in a battery of cognitive ability tests (Mackintosh 2011, 45); (Deary 2001, 222); (Deary 1998); (Lubinski 2004, 98)."
  },
  {
    "objectID": "1tooabstract.html#the-chc-model",
    "href": "1tooabstract.html#the-chc-model",
    "title": "1  What is intelligence?",
    "section": "1.3 The CHC model",
    "text": "1.3 The CHC model\nThe Cattell-Horn-Carroll (CHC) theory of intelligence is a prominent model that attempts to explain the structure of cognitive abilities. The theory proposes that intelligence is composed of a hierarchical structure of three strata: the general factor (g factor), broad abilities, and narrow abilities.\n\n\n\nCattell–Horn–Carroll theory of intelligence\n\n\nAt the top of the hierarchy is the g factor, which represents a general factor of intelligence that is common to all cognitive tasks. The broad abilities, located in the middle stratum, are a group of abilities that are less general than the g factor but still encompass a range of related cognitive tasks. Examples of broad abilities include fluid reasoning, crystallized intelligence, processing speed, and working memory. Finally, the narrow abilities, located at the bottom stratum, are specific abilities that are highly specialized and task-specific.\nThe CHC theory supports the existence of the g factor by showing that performance on different cognitive tasks is highly correlated, indicating a common underlying factor that influences performance across tasks. In addition, factor analyses of a wide range of intelligence tests consistently reveal a strong first factor that represents the g factor.\nMoreover, the CHC theory provides a framework for understanding the relationships between the g factor and other cognitive abilities. For example, fluid reasoning is considered a broad ability that is strongly related to the g factor and is involved in abstract thinking and problem-solving. Crystallized intelligence, on the other hand, is a broad ability that represents the application of knowledge and skills acquired through experience and education and is less strongly related to the g factor.\nBecause it has an impressive body of empirical support in the research literature (e.g., developmental, neurocognitive, outcome‐criterion), this model of intelligence is the one that psychologists predominantly use. It is used extensively as the foundation for selecting, organizing, and interpreting tests of intelligence and cognitive abilities. (Alfonso et al. 2005);(Beal 2006);(McGrew et al. 2005);(Schneider et al. 2012) Additionally, this model is the most comprehensive and empirically supported psychometric theory of the structure of cognitive abilities to date. (Flanagan and Dixon 2014)\nCHC is the best model from exploratory factor analysis (J. B. Carroll, B, and Press 1993) and is confirmed by confirmatory factor analysis (Gustafsson 1984); (J. Carroll 2003) (Jewsbury, Bowden, and Duff 2017). Confirmatory factor analysis showed that purposely different IQ batteries (CAB, Hawaii Battery, WAIS, etc) were analyzed, and it turned out that the g factors computed from the respective tests were statistically indistinguishable from one another, despite the fact that the tests tapped into partly different sets of abilities. (Johnson et al. 2004); (Johnson, Nijenhuis, and Bouchard 2008)\n\n\n\n\nAlfonso, V. C., D. P. Flanagan, S. Radwan, D. P. Flanagan, and P. L. Harrison. 2005. “The Impact of the Cattell-Horn-Carroll Theory on Test Development and Interpretation of Cognitive and Academic Abilities.” In Contemporary Intellectual Assessment: Theories, Tests, and Issues, 185–202.\n\n\nBeal, A Lynne. 2006. “Review of Contemporary Intellectual Assessment, Theories, Tests and Issues.”\n\n\nCarroll, J. B., C. J. B, and Cambridge University Press. 1993. Human Cognitive Abilities: A Survey of Factor-Analytic Studies. Human Cognitive Abilities: A Survey of Factor-Analytic Studies, no. 1. Cambridge University Press. https://books.google.com/books?id=jp9dt4\\_0\\_cIC.\n\n\nCarroll, John. 2003. “The Higher-Stratum Structure of Cognitive Abilities: Current Evidence Supports g and about Ten Broad Factors.” In The Scientific Study of General Intelligence: Tribute to Arthur R. Jensen, 5–21. https://doi.org/10.1016/B978-008043793-4/50036-2.\n\n\nDeary, Ian. 1998. “Differences in Mental Abilities.” BMJ (Clinical Research Ed.) 317 (7174): 1701–3. https://doi.org/10.1136/bmj.317.7174.1701.\n\n\n———. 2001. Intelligence: A Very Short Introduction. https://doi.org/10.1093/actrade/9780192893215.001.0001.\n\n\nFlanagan, Dawn P., and Shauna G. Dixon. 2014. “The Cattell-Horn-Carroll Theory of Cognitive Abilities.” In Encyclopedia of Special Education. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118660584.ese0431.\n\n\nGustafsson, Jan-Eric. 1984. “A Unifying Model for the Structure of Intellectual Abilities.” Intelligence 8 (3): 179–203. https://doi.org/https://doi.org/10.1016/0160-2896(84)90008-4.\n\n\nJewsbury, Paul, Stephen Bowden, and Kevin Duff. 2017. “The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment.” Journal of Psychoeducational Assessment 35 (6): 547–67. https://doi.org/10.1177/0734282916651360.\n\n\nJohnson, Wendy, Thomas J Bouchard, Robert F Krueger, Matt McGue, and Irving I Gottesman. 2004. “Just One g: Consistent Results from Three Test Batteries.” Intelligence 32 (1): 95–107. https://doi.org/https://doi.org/10.1016/S0160-2896(03)00062-X.\n\n\nJohnson, Wendy, Jan te Nijenhuis, and Thomas J. Bouchard. 2008. “Still Just 1 g: Consistent Results from Five Test Batteries.” Intelligence 36 (1): 81–95. https://doi.org/https://doi.org/10.1016/j.intell.2007.06.001.\n\n\nLubinski, David. 2004. “Introduction to the Special Section on Cognitive Abilities: 100 Years After Spearman’s (1904) \"’General Intelligence,’ Objectively Determined and Measured\".” Journal of Personality and Social Psychology 86 (1): 96–111. https://doi.org/10.1037/0022-3514.86.1.96.\n\n\nMackintosh, Nicholas. 2011. IQ and Human Intelligence. OUP Oxford. https://books.google.com/books?id=BcKcAQAAQBAJ.\n\n\nMcGrew, Kevin S, DP Flanagan, JL Genshaft, and PL Harrison. 2005. Contemporary Intellectual Assessment: Theories, Tests, and Issues. The Guilford Press New York, NY, USA:\n\n\nSchneider, W Joel, Kevin S McGrew, DP Flanagan, and PL Harrison. 2012. “Contemporary Intellectual Assessment: Theories, Tests, and Issues.” Institute for Applied Psychometrics (IAP)."
  },
  {
    "objectID": "3testbias.html#evidences-of-test-bias",
    "href": "3testbias.html#evidences-of-test-bias",
    "title": "3  Aren’t intelligence tests biased?",
    "section": "3.1 Evidences of test bias",
    "text": "3.1 Evidences of test bias\nThere has been much debate around the potential bias of intelligence tests. However, research has consistently shown that intelligence tests, when administered and scored correctly, are not biased against any particular group or population.\nIn fact, intelligence tests are designed to be culturally fair and measure cognitive abilities that are not tied to any specific cultural or socioeconomic group. The tests are created using a rigorous process that involves extensive pilot testing, item analysis, and psychometric evaluation.\nExpert opinion finds that “the issue of test bias is scientifically dead” (Hunter and Schmidt 2000, 151) “There exists, in contrast to the position of the late 1960s, a substantial body of research addressing the cultural test bias hypothesis, and scholars have extensively reviewed this literature in book and chapter-length forums (A. R. Jensen 1980); (Cecil R. Reynolds 1995); (Cecil R. Reynolds 1998); (Cecil R. Reynolds and Brown 1984). Essentially, these reviews have concluded that well-constructed, reliable, well-standardized psychological tests are not biased against native-born American racial or ethnic minorities. Studies of cultural bias in testing do occasionally reveal bias in some aspects of validity, but the bias is more likely to favor minorities in a predictive context” (Cecil R. Reynolds 2000);(Cecil R. Reynolds and Suzuki 2012)\nIn 1982, the National Research Council and the National Academy of Sciences found that “ability tests predict equally well for all groups of test takers. Research evidence does not support the notion that tests systematically underpredict the performance of minority group members” (Wigdor 1982) In 1994, Mainstream Science on Intelligence, a statement issued by several researchers, finds that “Intelligence tests are not culturally biased against American blacks or other native-born, English-speaking peoples in the U.S. Rather, IQ scores predict equally accurately for all such Americans, regardless of race and social class. Individuals who do not understand English well can be given either a nonverbal test or one in their native language.” (Gottfredson 1997). In 1996, the American Psychological Association found that “considered as predictors of future performance, the tests do not seem to be biased against African Americans.” (Neisser et al. 1996). (Reeve and Charles 2008) found that there is a consensus among intelligence experts that cognitive ability tests are not bias against minority groups\nAnd most important of all: logical and empirical examination: If intelligence tests merely measure knowledge or conformity to Western middle-class culture is the fact that the racial group with the highest average on these tests is not Europeans, but East Asians; this has been true since the 1920s (Goodenough 1926). If intelligence tests measure acculturation to Western culture, then indigenous communities who have had more contact with Europeans should score higher than people in communities in the same nation who have had less contact. However, this is not the case (Porteus 1965).\nContent of many test formats (e.g., matrix tests, digit span) contains very little information that is unique to Western culture. It is not clear how numbers or geometric patterns are special to Western middle-class individuals. Expert rated culturally loaded test items do not contribute to an increase in the Black/White divide. (Arthur R. Jensen and McGurk 1987) If test items were culturally loaded, (especially to a degree that would significantly change test results) this would mean test items ranked by difficulty would not correlate well by race. However, the opposite is true. (Cecil R. Reynolds and Suzuki 2012) Correlation of rank order difficulty between cognitive batteries is nearly perfect among Blacks and Whites."
  },
  {
    "objectID": "3testbias.html#evidences-regarding-stereotype-threat",
    "href": "3testbias.html#evidences-regarding-stereotype-threat",
    "title": "3  Aren’t intelligence tests biased?",
    "section": "3.2 Evidences regarding stereotype threat",
    "text": "3.2 Evidences regarding stereotype threat\nRacial bias\nThe literature on stereotype threat is not great. (Schimmack 2017) We have even had papers that are completely fabricated (probably for political motivation). (Flores et al. 2021) We have a few meta analyses on this phenomenon. Publication bias plagues many of these studies and when factored for, the effect sizes decrease. (Flore and Wicherts 2015);(Shewach, Sackett, and Quint 2019), a meta-analysis of 212 experimental studies on stereotype threat, which included a total of more than 10,000 adult participants effects (the largest to date) ‘range[d] from negligible to small’ If we assume stereotype threat is an actual possible influential factor, we would actually expect Black people to have increased scores as they measure higher than whites on various psychological metrics\nCompared to Whites, Blacks have higher self-esteem (Twenge and Crocker 2002), Lower suicide rates (Curtin, Brown, and Jordan 2022), Lower rates of stress (Krueger, Saint Onge, and Chang 2011), Lower risk for panic disorder, Lower risk for generalized anxiety order, Lower risk for social phobia (Breslau et al. 2006), Lower rates for depressive disorder (Riolo et al. 2005), Lower overall rate of mental disorders (Coleman et al. 2016). Since Black people are more confident and mentally healthy, it doesn’t seem like stereotype threat could be a relevant factor. In conclusion: the evidence points to there being no cultural bias in IQ testing and there doesn’t seem to be enough methodologically robust literature to make a certain claim on stereotype threat (though more recent analyses that factor for publication bias show null effects). The burden of proof rests on those who claim this to significantly affect the gap between blacks and whites.\n\n\n\n\nBreslau, Joshua, Sergio Aguilar-Gaxiola, Kenneth S. Kendler, Maxwell Su, David Williams, and Ronald C. Kessler. 2006. “Specifying Race-Ethnic Differences in Risk for Psychiatric Disorder in a USA National Sample.” Psychological Medicine 36 (1): 57–68. https://doi.org/10.1017/S0033291705006161.\n\n\nColeman, Karen J., Christine Stewart, Beth E. Waitzfelder, John E. Zeber, Leo S. Morales, Ameena T. Ahmed, Brian K. Ahmedani, et al. 2016. “Racial-Ethnic Differences in Psychiatric Diagnoses and Treatment Across 11 Health Care Systems in the Mental Health Research Network.” Psychiatric Services 67 (7): 749–57. https://doi.org/10.1176/appi.ps.201500217.\n\n\nCurtin, Sally C, Kamiah A Brown, and Mariah E Jordan. 2022. “Suicide Rates for the Three Leading Methods by Race and Ethnicity : United States, 2000–2020.” NCHS Data Brief ; No. 450. https://dx.doi.org/10.15620/cdc:121798; National Center for Health Statistics.\n\n\nFlore, Paulette C., and Jelte M. Wicherts. 2015. “Does Stereotype Threat Influence Performance of Girls in Stereotyped Domains? A Meta-Analysis.” Journal of School Psychology 53 (1): 25–44. https://doi.org/https://doi.org/10.1016/j.jsp.2014.10.002.\n\n\nFlores, AJ, TA Chavez, N Bolger, and BJ Casad. 2021. “Retraction Notice.” Personality and Social Psychology Bulletin 47 (1): 161–61. https://doi.org/10.1177/0146167220973962.\n\n\nGoodenough, FL. 1926. “Racial Differences in the Intelligence of School Children.” Journal of Experimental Psychology 9 (4): 388–97. https://doi.org/10.1037/h0073325.\n\n\nGottfredson, Linda S. 1997. “Mainstream Science on Intelligence: An Editorial with 52 Signatories, History, and Bibliography.” Intelligence 24 (1): 13–23. https://doi.org/https://doi.org/10.1016/S0160-2896(97)90011-8.\n\n\nHunter, John E, and Frank L Schmidt. 2000. “Racial and Gender Bias in Ability and Achievement Tests: Resolving the Apparent Paradox.” Psychology, Public Policy, and Law 6 (1): 151–58. https://doi.org/10.1037/1076-8971.6.1.151.\n\n\nJensen, A. R. 1980. Bias in Mental Testing. Free Press. https://books.google.com/books?id=wJR9AAAAMAAJ.\n\n\nJensen, Arthur R., and Frank C. J. McGurk. 1987. “Black-White Bias in ‘Cultural’ and ‘Noncultural’ Test Items.” Personality and Individual Differences 8 (3): 295–301. https://doi.org/https://doi.org/10.1016/0191-8869(87)90029-8.\n\n\nKrueger, Patrick M, Jarron M Saint Onge, and Virginia W Chang. 2011. “Race/Ethnic Differences in Adult Mortality: The Role of Perceived Stress and Health Behaviors.” Social Science & Medicine 73 (9): 1312–22. https://doi.org/10.1016/j.socscimed.2011.08.007.\n\n\nNeisser, Ulric, Gwyneth Boodoo, Thomas J. Bouchard Jr., A. Wade Boykin, Nathan Brody, Stephen J. Ceci, Diane F. Halpern, et al. 1996. “Intelligence: Knowns and Unknowns.” American Psychologist 51 (2): 77–101. https://doi.org/10.1037/0003-066X.51.2.77.\n\n\nPorteus, SD. 1965. Porteus Maze Test; Fifty Years’ Application. Pacific Books.\n\n\nReeve, Charlie L., and Jennifer E. Charles. 2008. “Survey of Opinions on the Primacy of g and Social Consequences of Ability Testing: A Comparison of Expert and Non-Expert Views.” Intelligence 36 (6): 681–88. https://doi.org/https://doi.org/10.1016/j.intell.2008.03.007.\n\n\nReynolds, Cecil R. 2000. “Why Is Psychometric Research on Bias in Mental Testing so Often Ignored?” Psychology, Public Policy, and Law 6 (1): 144–50. https://doi.org/10.1037/1076-8971.6.1.144.\n\n\nReynolds, Cecil R. 1995. “Test Bias and the Assessment of Intelligence and Personality.” In International Handbook of Personality and Intelligence, edited by Donald H. Saklofske and Moshe Zeidner, 545–73. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4757-5571-8_25.\n\n\n———. 1998. “10.03 - Cultural Bias in Testing of Intelligence and Personality.” In Comprehensive Clinical Psychology, edited by Alan S. Bellack and Michel Hersen, 53–92. Oxford: Pergamon. https://doi.org/https://doi.org/10.1016/B0080-4270(73)00105-X.\n\n\nReynolds, Cecil R., and Robert T. Brown. 1984. “Bias in Mental Testing.” In Perspectives on Bias in Mental Testing, edited by Cecil R. Reynolds and Robert T. Brown, 1–39. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4684-4658-6_1.\n\n\nReynolds, Cecil R., and Lisa A. Suzuki. 2012. “Bias in Psychological Assessment.” In Handbook of Psychology, Second Edition. John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118133880.hop210004.\n\n\nRiolo, Stephanie A., Tuan Anh Nguyen, John F. Greden, and Cheryl A. King. 2005. “Prevalence of Depression by Race/Ethnicity: Findings from the National Health and Nutrition Examination Survey III.” American Journal of Public Health 95 (6): 998–1000. https://doi.org/10.2105/AJPH.2004.047225.\n\n\nSchimmack, Ulrich. 2017. “Hidden Figures: Replication Failures in the Stereotype Threat Literature.” https://replicationindex.com/2017/04/07/hidden-figures-replication-failures-in-the-stereotype-threat-literature/.\n\n\nShewach, Oren R, Paul R Sackett, and Sander Quint. 2019. “Stereotype Threat Effects in Settings with Features Likely Versus Unlikely in Operational Test Settings: A Meta-Analysis.” Journal of Applied Psychology 104 (12): 1514–34. https://doi.org/10.1037/apl0000420.\n\n\nTwenge, Jean M, and Jennifer Crocker. 2002. “Race and Self-Esteem: Meta-Analyses Comparing Whites, Blacks, Hispanics, Asians, and American Indians and Comment on Gray-Little and Hafdahl (2000).” Psychological Bulletin 128 (3): 371–408. https://doi.org/10.1037/0033-2909.128.3.371.\n\n\nWigdor, Alexandra K. 1982. “Ability Testing: Uses, Consequences, and Controversies.” Educational Measurement: Issues and Practice 1 (3): 6–8. https://doi.org/https://doi.org/10.1111/j.1745-3992.1982.tb00659.x."
  }
]